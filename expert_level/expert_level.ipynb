{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a085be33-7c88-4a4f-a698-4f033a023888",
   "metadata": {},
   "source": [
    "# How to read that document\n",
    "\n",
    "**You're intended to go through the low loss intermediate level notebook before completing that practical. Here we expect you to know how to perform data preprocessing and slicing.**\n",
    "\n",
    "In this practical, you will learn how to use basic machine learning algorithm for the analysis of EELS data. If you are note familiar with Principal Component Analysis (PCA), we strongly suggest you to read the following. The next few questions will guide you through this practical : \n",
    "\n",
    "- How many components should you use to describe the data ? How many of them correspond to a physical signature ?\n",
    "- How do the loadings obtained using PCA compare with the maps obtained by hand ?\n",
    "- How do the loadings obtained using NMF compare with the maps obtained by hand ?\n",
    "\n",
    "This notebook is largely inspired by the hyperspy documentation: http://hyperspy.org/hyperspy-doc/current/index.html\n",
    "\n",
    "The data were kindly given by: Hugo Lourenço-Martins.\n",
    "\n",
    "## Warning\n",
    "\n",
    "**The methods presented here are very powerful but it may sometimes lead to wrong interpretations. If you plan to use them in your work we strongly recommend you to double check for the validity of the approach.** See for example: \n",
    "\n",
    "LICHTERT, Stijn et VERBEECK, Jo. Statistical consequences of applying a PCA noise filter on EELS spectrum images. Ultramicroscopy, 2013, vol. 125, p. 35-42."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bfaadf-ad77-43d7-ba9b-a454e4b877e1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# EELS statistics\n",
    "\n",
    "Most of the tools in the hands of the analyst are based on statistical priors on the data. Thus, in this section we will describe the statistics of the EELS data.\n",
    "\n",
    "The EELS processes are Poissonian in nature. Indeed, we observe a flow of electron over a given period of time and the inelastic scattering events are (in a good approximation) statistically independant from each other. It is often referred as shot noise.\n",
    "\n",
    "Then the detection system can add up different kinds of noise such as beam jittering or dark current. We will detail here only the detector related noises.\n",
    "\n",
    "## CCD detectors\n",
    "\n",
    "The variance of the statistical noise of the CCD is given by the following formula : \n",
    "\n",
    "$$ Var(J(E)) = g + p J(E) $$\n",
    "\n",
    "Where $J(E)$ is the detected electron current, following a Poisson statistics. $p$ is a conversion proportionality constant and $g$ is an additive gaussian noise originating from read-out noise and dark current. The variance is not constant over the channels of the detector.\n",
    "\n",
    "## Direct detection\n",
    "\n",
    "The direct electron detectors are mostly limited by shot noise, thus the noise variance becomes : \n",
    "\n",
    "$$ Var(J(E)) = p J(E) $$\n",
    "\n",
    "The absence of $g$ drastically improve the performances of these detectors (especially at low doses). The variance is still not constant over the channels of the detectors.\n",
    "\n",
    "In general, the data acquired using direct detection are more straightforward to analyze.\n",
    "\n",
    "### References\n",
    "\n",
    "EGERTON, Ray F. Electron energy-loss spectroscopy in the electron microscope. Springer Science & Business Media, 2011.\n",
    "\n",
    "De la Peña Manchón, Francisco J. Advanced methods for Electron Energy Loss Spectroscopy core-loss analysis, PhD thesis, 2010\n",
    "\n",
    "HART, James L., LANG, Andrew C., LEFF, Asher C., et al. Direct detection electron energy-loss spectroscopy: a method to push the limits of resolution and sensitivity. Scientific reports, 2017, vol. 7, no 1, p. 1-14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3560e9-13ca-4e98-9357-0ca52282c784",
   "metadata": {},
   "source": [
    "# PCA\n",
    "\n",
    "Principal Component Analysis is a machine learning algorithm that can be used for data analysis or for denoising of multidimensional data. It is based on statistical principles.\n",
    "\n",
    "## Principle\n",
    "\n",
    "The data below are represented using two main axes, x and y. Each of those axes correspond to some variance of the data as it can be seen in the projection of the data on those axes (thick redlines). \n",
    "\n",
    "![image](images/random_data.png)\n",
    "\n",
    "The PCA will reorganise the axes on which the data are represented. The axes are rotated so that they correspond to gradually decreasing variance of the data. It means that the first axis (arrow parallel the line) is represents the highest variance in the data (Thick blue line) and the second axis (arrow perpendicular to the line) represents lower variance (thick green line). \n",
    "\n",
    "![image](images/random_data_PCA.png)\n",
    "\n",
    "For the data analysis, this reorganisation of the axes is useful. With the new representation we easily get that the main feature of the signal is the straight line and that the axis perpendicular to it represents mainly noise. **In general, after PCA you should determine which are the relevant axes (or components) describing your signal and discard the noisy ones.**\n",
    "\n",
    "## The impact of noise statistics\n",
    "\n",
    "The data below were generated with Poisson noise. The variance is not constant for all the data points. The reorganisation of the axes will not occur in the same way. Therefore, a correction of this effect is required. That is also why it is important to understand the statistics of the data you analyze.\n",
    "\n",
    "![image](images/poisson_data.png)\n",
    "\n",
    "⭐ When applying a least-square fit on data, the same principle applies. It is technically incorrect to fit data exhibiting Poisson statistics using a least-square method.\n",
    "\n",
    "## Increasing the dimension of the data\n",
    "\n",
    "The PCA principles apply even with data of higher dimension. For spectrum images, they can be represented as a collection of spectra (N pixels spectra). A point in 2D space corresponds to 2 coordinates (x,y). A point in 3D space corresponds to 3 coordinates (x,y,z). A spectrum can be seen as a point in E-dimensional space ($I_1, I_2, ..., I_E$).\n",
    "\n",
    "![image](images/flat_spim.png)\n",
    "\n",
    "PCA is going to decompose the data in two matrices. The first matricx called factors contains the vectors of the new representation, each column contains one spectrum-like axis. Each row of the second matrix (called loadings) correspond to the intensity of a given axis of the new representation. In the previous part with the line, the first line of the loadings will give out where each point is on the line. \n",
    "\n",
    "![image](images/decomposition.png)\n",
    "\n",
    "For example, a spectral signature with high variance (such as the zero-loss peak) will have it's own axis in the new representation. Thus, there will be a corresponding loading describing the spatial evolution of the zero-loss peak intensity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f35cd95-94a1-43e9-bc80-e75d2e14e904",
   "metadata": {},
   "source": [
    "# Applying PCA\n",
    "\n",
    "**We assume here that the data you load are aligned in energy and deconvoluted. Which is not the case in the example below.**\n",
    "\n",
    "We use the `spim.decomposition` function to perform the PCA. Using the positional argument `spim.decomposition(True)`, the poissonian nature of the noise is taken into account (within some approximation). \n",
    "\n",
    "⭐ For fully taking into account the poisson statistics, you will need to use the maximum likelihood formulation of the algorihtm. This is more computationnally expensive though.\n",
    "\n",
    "### References \n",
    "\n",
    "KEENAN, Michael R. et KOTULA, Paul G. Accounting for Poisson noise in the multivariate analysis of ToF‐SIMS spectrum images. Surface and Interface Analysis: An International Journal devoted to the development and application of techniques for the analysis of surfaces, interfaces and thin films, 2004, vol. 36, no 3, p. 203-212."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa82a70e-8aae-4d20-8480-3f4d0da55b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "import hyperspy.api as hs\n",
    "\n",
    "spim = hs.load('spim_low_loss.hspy')\n",
    "spim.decomposition(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238a5c5c-82ff-43a3-b262-b1ad82a63d77",
   "metadata": {},
   "source": [
    "## Explained variance ratio\n",
    "\n",
    "To help you determine the relevant components in your dataset, there is a tool in hyperspy to plot how much variance of the data correspond to each axis of the new representation. \n",
    "\n",
    "This plot organises the axes by decreasing variance. Often, this plot as an elbow shape. As a rule of thumb, the number of relevant components is approximately given by the position of the elbow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3478c6-6214-41c6-a603-d0d86b0f8883",
   "metadata": {},
   "outputs": [],
   "source": [
    "spim.plot_explained_variance_ratio()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fade2872-303d-4a13-b7c4-6e81d3281904",
   "metadata": {},
   "source": [
    "## Plotting the results\n",
    "\n",
    "You can plot the factors and loadings using `spim.plot_decomposition_factors` and `spim.plot_decomposition_loadings`. Here are a few useful options for display: \n",
    "\n",
    "- You can use an integer `spim.plot_decomposition_factors(3)`, which will display the 3 first factors. Or you can use a list `spim.plot_decomposition_factors([1,3,5])` which will display the factors 1, 3 and 5 only.\n",
    "\n",
    "- You can plot everything in separated windows using the `same_window = False` keyword argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9eee49-0e03-4d70-96ae-4bc6235a881c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spim.plot_decomposition_factors(3,same_window = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ff01f6-ddbe-4800-8482-4ee78103c92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spim.plot_decomposition_loadings(3,same_window = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb6b175-178b-40b7-a195-c9d16141d380",
   "metadata": {},
   "source": [
    "# NMF\n",
    "\n",
    "## Principles\n",
    "\n",
    "The Non-Negative Matrix Factorization is somewhat similar to PCA. It decomposes the data into factors and loadings, although they are not organized according to decreasing variance. The main difference is that the factors and loadings are constrained to positive values. \n",
    "\n",
    "We activate the NMF using the keyword argument `algorithm = 'NMF'`. The calculation is longer we limit beforehand the number of components using the keyword argument `output_dimension = n`, where `n` is the integer you choose. The algorithm might not converge with the default amount of iterations, you can use `max_iter=` to increase it.\n",
    "\n",
    "**The PCA modifies the spim object. It is recommended to reload the data before performing NMF.**\n",
    "\n",
    "⭐ Hyperspy has a flexible enough interface. You can use many different decomposition algorithms such as the ones of scikit-learn with more or less the same syntax. For example, the value of `algorithm=` can be any object that implements the `fit` and `transform` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c87f13-7720-4e69-8cd7-b3cdb8f06152",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "import hyperspy.api as hs\n",
    "\n",
    "spim = hs.load('spim_low_loss.hspy')\n",
    "spim.decomposition(True,algorithm = 'NMF',output_dimension = 3,max_iter = 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7880b8c-7002-40e1-8965-5b045e434f79",
   "metadata": {},
   "source": [
    "## Plotting the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82744d68-a2e6-4ec6-842f-7c41fc2d9dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "spim.plot_decomposition_factors(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9225344-9d43-4090-86eb-e6ae118a64a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spim.plot_decomposition_loadings(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qem",
   "language": "python",
   "name": "qem"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
